{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP9G5BO2sGDV",
        "outputId": "c2fe37cd-8e2b-4220-fcfe-8153e548a8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "5k370l4csTr6"
      },
      "outputs": [],
      "source": [
        "# Function to download and extract the dataset\n",
        "def download_and_extract_dataset(dataset_link, extract_to):\n",
        "    # Download the dataset zip file\n",
        "    dataset_zip_url = dataset_link + \"&dl=1\"\n",
        "    dataset_zip_path = os.path.join(extract_to, \"dataset.zip\")\n",
        "    wget.download(dataset_zip_url, out=dataset_zip_path)\n",
        "    print(\"Dataset downloaded successfully.\")\n",
        "\n",
        "    # Extract the dataset\n",
        "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"Dataset extracted successfully.\")\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_and_preprocess_data(data_dir):\n",
        "    labels = []\n",
        "    audio_paths = []\n",
        "\n",
        "    # Traverse the directory structure to extract labels and audio paths\n",
        "    for label in os.listdir(data_dir):\n",
        "        label_dir = os.path.join(data_dir, label)\n",
        "        if os.path.isdir(label_dir):\n",
        "            for audio_file in os.listdir(label_dir):\n",
        "                audio_path = os.path.join(label_dir, audio_file)\n",
        "                audio_paths.append(audio_path)\n",
        "                labels.append(label)\n",
        "\n",
        "    print(\"Number of samples:\", len(audio_paths))\n",
        "    return labels, audio_paths\n",
        "\n",
        "# Function to preprocess audio data (MFCC extraction)\n",
        "def preprocess_audio(audio_path, sequence_length):\n",
        "    audio, sr = librosa.load(audio_path, sr=None)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    # Pad or truncate MFCCs to match the sequence length\n",
        "    if mfccs.shape[1] < sequence_length:\n",
        "        pad_width = sequence_length - mfccs.shape[1]\n",
        "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mfccs = mfccs[:, :sequence_length]\n",
        "    return mfccs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "HYg5J1QyshIf"
      },
      "outputs": [],
      "source": [
        "# Function to build the ASR model\n",
        "def build_model(input_shape, output_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape),\n",
        "        tf.keras.layers.LSTM(128, return_sequences=False),\n",
        "        tf.keras.layers.Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "FnMLLUz_sk6q"
      },
      "outputs": [],
      "source": [
        "# Function to train the ASR model\n",
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "eXSkHFJJsojy"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the ASR model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRRi_dnHsrte"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    dataset_url = \"https://www.dropbox.com/scl/fo/jvcx6dwpvuwaiboijg34d/ALVdJuoj1IyybQJ2SC3thHc?rlkey=px94zhss4kr66c619q1jfqwzt&st=9jfmfgun\"\n",
        "    extract_to = \"data\"\n",
        "    data_dir = \"data\"\n",
        "    sequence_length = 1000\n",
        "\n",
        "    # Download and extract the dataset\n",
        "    # download_and_extract_dataset(dataset_url, extract_to)\n",
        "\n",
        "    # Preprocess audio data and labels\n",
        "    labels, audio_paths = load_and_preprocess_data(extract_to)\n",
        "\n",
        "    # Convert labels to a NumPy array\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Process features\n",
        "    features = []\n",
        "    for audio_path in audio_paths:\n",
        "        try:\n",
        "            feature = preprocess_audio(audio_path, sequence_length)\n",
        "            features.append(feature)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {e}\")\n",
        "\n",
        "    features = np.array(features)\n",
        "    print(\"Number of samples processed:\", len(features))\n",
        "\n",
        "    # Encode labels\n",
        "    label_to_index = {label: index for index, label in enumerate(np.unique(labels))}\n",
        "    labels_encoded = [label_to_index[label] for label in labels]\n",
        "    num_classes = len(label_to_index)\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    labels_one_hot = to_categorical(labels_encoded, num_classes=num_classes)\n",
        "\n",
        "    # Split data into training, validation, and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build the model\n",
        "    input_shape = features[0].shape\n",
        "    num_classes = len(np.unique(labels))  # Calculate the number of classes\n",
        "    model = build_model(input_shape=input_shape, output_dim=num_classes)\n",
        "\n",
        "    # Train the model\n",
        "    history = train_model(model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    # Visualize training/validation loss and accuracy\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}